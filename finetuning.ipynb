{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "env = load_dotenv('token.env')\n",
    "api_token = os.getenv('api_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f'Number of GPUs available: {num_gpus}')\n",
    "\n",
    "    # Print the name of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "else:\n",
    "    print('No GPUs available, running on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-2b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token = api_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,device_map = 'cuda', token = api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(\"data\")\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_directory = \"data\"\n",
    "\n",
    "datasets = DatasetDict()\n",
    "for i,file in enumerate(file_names):\n",
    "    with open(f\"{data_directory}/{file}\", 'r') as f:\n",
    "        data = f.read()\n",
    "    datasets[i] = Dataset.from_dict({'text': [data]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Get the list of datasets from the DatasetDict\n",
    "datasets_list = list(datasets.values())\n",
    "\n",
    "# Concatenate the datasets into a single Dataset\n",
    "dataset = concatenate_datasets(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Narrator]: Within the heart of the enchanted forest, where ancient trees stand sentinel and the air is thick with the scent of magic, a fearsome creature prowls, its eyes burning with primal rage. With a deafening roar, it emerges from the shadows, its massive form casting a menacing shadow over the forest floor.\n",
      "\n",
      "[Player]: With a steady hand, I draw my weapon, prepared to confront the beast and defend myself.\n",
      "\n",
      "[Narrator]: The clash of steel fills the air as you engage in a fierce battle with the creature, each strike ringing out like thunder amidst the tranquil forest. Despite its ferocity, you stand your ground, determined to emerge victorious against this formidable adversary.\n",
      "\n",
      "[Player]: I fight with all my strength, seeking out weaknesses in the beast's defenses as I strive to gain the upper hand.\n",
      "\n",
      "[Narrator]: With each exchange of blows, you feel the weight of the creature's relentless assault bearing down upon you, testing your resolve and pushing you to the brink of exhaustion. Yet, despite the odds stacked against you, you refuse to yield, drawing upon every ounce of courage and determination within you to press onward.\n",
      "\n",
      "[Player]: With a surge of adrenaline, I unleash a barrage of strikes, aiming to overpower the creature and claim victory in this deadly contest.\n",
      "\n",
      "[Narrator]: With a mighty roar, you deliver a decisive blow that finds its mark, sending the beast staggering backward with a pained growl. Sensing an opportunity, you press your advantage, driving forward with renewed determination.\n",
      "\n",
      "[Player]: I summon all of my strength for one final strike, aiming to deliver the decisive blow that will end the battle once and for all.\n",
      "\n",
      "[Narrator]: With a mighty swing, your weapon connects with the beast's vulnerable spot, striking true and sending it crashing to the forest floor with a thunderous roar. As the echoes of battle fade away, you stand victorious amidst the ancient trees, your heart pounding with a mixture of relief and triumph.\n",
      "\n",
      "[Player]: I breathe a sigh of relief, grateful for my victory over the fearsome creature that once threatened to overwhelm me.\n",
      "\n",
      "[Narrator]: With the creature defeated, you take a moment to catch your breath, the stillness of the forest enveloping you like a comforting embrace. As you stand amidst the ancient trees, you feel a sense of pride swell within you, knowing that you have proven yourself capable of overcoming even the most daunting of challenges.\n",
      "\n",
      "[Game Over]: In the heart of the enchanted forest, you have emerged victorious against a formidable adversary, your courage and resilience shining bright amidst the darkness. May your triumph serve as a beacon of hope for all who dare to tread the paths of the mystical realm, guiding them ever onward on their own quests for glory and adventure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = '[Game Start]\\n\\n'\n",
    "    example['text'] = start_prompt + example['text']\n",
    "    example['text'] = tokenizer(example['text'], padding = 'max_length',max_length=2500,truncation= True , return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    return example\n",
    "\n",
    "# def tokenize_function(example):\n",
    "#     start_prompt = '[Game Start]\\n\\n'\n",
    "#     example['text'] = tokenizer(start_prompt + example['text'])\n",
    "#     return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 39/39 [00:00<00:00, 496.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tk_dataset = dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,value in enumerate(tk_dataset['text'][7]['input_ids'][0]):\n",
    "#     if value != 0:\n",
    "#         print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tk_dataset = tk_dataset.remove_columns(['text'])\n",
    "# tk_dataset = tk_dataset.rename_column('token', 'input_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 39\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "save_steps = 10\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 500\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    # gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    # save_steps=save_steps,\n",
    "    # logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    # max_grad_norm=max_grad_norm,\n",
    "    # max_steps=max_steps,\n",
    "    # warmup_ratio=warmup_ratio,\n",
    "    # group_by_length=True,\n",
    "    # gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adit/anaconda3/envs/gemma/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "train_args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[0;32m---> 11\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[1;32m     12\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtk_dataset,\n\u001b[1;32m     13\u001b[0m     args\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[1;32m     14\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m         gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     16\u001b[0m         warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     17\u001b[0m         max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     18\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-4\u001b[39m,\n\u001b[1;32m     19\u001b[0m         fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m         logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     21\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m         optim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaged_adamw_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m     \n\u001b[1;32m     24\u001b[0m     ),\n\u001b[1;32m     25\u001b[0m     peft_config\u001b[38;5;241m=\u001b[39mlora_config,\n\u001b[1;32m     26\u001b[0m     formatting_func\u001b[38;5;241m=\u001b[39mformatting_func,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "def formatting_func(example):\n",
    "    text = f\"<start_of_turn>user\\n{example['INSTRUCTION'][0]}<end_of_turn> <start_of_turn>model\\n{example['RESPONSE'][0]}<end_of_turn>\"\n",
    "    return [text]\n",
    "\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tk_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=150,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    \n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk_dataset['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  235309,\n",
       "  6242,\n",
       "  7248,\n",
       "  235307,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  1877,\n",
       "  692,\n",
       "  27418,\n",
       "  22583,\n",
       "  1280,\n",
       "  573,\n",
       "  115885,\n",
       "  9309,\n",
       "  235269,\n",
       "  573,\n",
       "  25619,\n",
       "  55868,\n",
       "  3131,\n",
       "  78608,\n",
       "  41471,\n",
       "  12136,\n",
       "  576,\n",
       "  2611,\n",
       "  578,\n",
       "  12425,\n",
       "  3054,\n",
       "  573,\n",
       "  9309,\n",
       "  6784,\n",
       "  235265,\n",
       "  1646,\n",
       "  798,\n",
       "  2375,\n",
       "  573,\n",
       "  87434,\n",
       "  4134,\n",
       "  57873,\n",
       "  1384,\n",
       "  2449,\n",
       "  692,\n",
       "  235269,\n",
       "  216571,\n",
       "  696,\n",
       "  573,\n",
       "  8566,\n",
       "  576,\n",
       "  22172,\n",
       "  1996,\n",
       "  27290,\n",
       "  120076,\n",
       "  71557,\n",
       "  573,\n",
       "  8195,\n",
       "  235265,\n",
       "  62122,\n",
       "  235269,\n",
       "  692,\n",
       "  2063,\n",
       "  4492,\n",
       "  476,\n",
       "  37003,\n",
       "  1570,\n",
       "  476,\n",
       "  75045,\n",
       "  27988,\n",
       "  12353,\n",
       "  1794,\n",
       "  692,\n",
       "  235269,\n",
       "  1277,\n",
       "  4628,\n",
       "  134775,\n",
       "  675,\n",
       "  671,\n",
       "  1156,\n",
       "  192464,\n",
       "  17273,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  102079,\n",
       "  5688,\n",
       "  573,\n",
       "  27988,\n",
       "  578,\n",
       "  20483,\n",
       "  1277,\n",
       "  157682,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  714,\n",
       "  27988,\n",
       "  235269,\n",
       "  78248,\n",
       "  476,\n",
       "  70318,\n",
       "  43314,\n",
       "  675,\n",
       "  476,\n",
       "  166962,\n",
       "  13618,\n",
       "  576,\n",
       "  8343,\n",
       "  235269,\n",
       "  13807,\n",
       "  692,\n",
       "  675,\n",
       "  39987,\n",
       "  235265,\n",
       "  9707,\n",
       "  1497,\n",
       "  28058,\n",
       "  485,\n",
       "  2095,\n",
       "  26725,\n",
       "  575,\n",
       "  573,\n",
       "  40567,\n",
       "  685,\n",
       "  665,\n",
       "  5011,\n",
       "  617,\n",
       "  1277,\n",
       "  2206,\n",
       "  235269,\n",
       "  37925,\n",
       "  48973,\n",
       "  692,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  15949,\n",
       "  970,\n",
       "  1634,\n",
       "  13708,\n",
       "  235269,\n",
       "  13051,\n",
       "  665,\n",
       "  604,\n",
       "  573,\n",
       "  27988,\n",
       "  577,\n",
       "  73931,\n",
       "  235269,\n",
       "  30706,\n",
       "  577,\n",
       "  8147,\n",
       "  476,\n",
       "  27322,\n",
       "  14584,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  714,\n",
       "  43314,\n",
       "  96236,\n",
       "  1277,\n",
       "  2206,\n",
       "  235269,\n",
       "  1277,\n",
       "  105566,\n",
       "  81887,\n",
       "  2691,\n",
       "  861,\n",
       "  163848,\n",
       "  1634,\n",
       "  235265,\n",
       "  1699,\n",
       "  476,\n",
       "  4318,\n",
       "  235269,\n",
       "  1104,\n",
       "  603,\n",
       "  476,\n",
       "  45530,\n",
       "  18499,\n",
       "  235269,\n",
       "  1492,\n",
       "  573,\n",
       "  27988,\n",
       "  22566,\n",
       "  921,\n",
       "  476,\n",
       "  4072,\n",
       "  733,\n",
       "  473,\n",
       "  1283,\n",
       "  578,\n",
       "  3261,\n",
       "  60183,\n",
       "  861,\n",
       "  1634,\n",
       "  161436,\n",
       "  235265,\n",
       "  1165,\n",
       "  4930,\n",
       "  692,\n",
       "  791,\n",
       "  19978,\n",
       "  1277,\n",
       "  7930,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  102079,\n",
       "  21339,\n",
       "  573,\n",
       "  43314,\n",
       "  235303,\n",
       "  235256,\n",
       "  28058,\n",
       "  235269,\n",
       "  18360,\n",
       "  577,\n",
       "  4024,\n",
       "  12429,\n",
       "  675,\n",
       "  665,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  714,\n",
       "  43314,\n",
       "  44646,\n",
       "  1277,\n",
       "  4628,\n",
       "  9249,\n",
       "  27595,\n",
       "  235269,\n",
       "  2572,\n",
       "  1544,\n",
       "  575,\n",
       "  861,\n",
       "  12446,\n",
       "  6437,\n",
       "  235265,\n",
       "  1165,\n",
       "  8149,\n",
       "  692,\n",
       "  791,\n",
       "  10311,\n",
       "  476,\n",
       "  6653,\n",
       "  675,\n",
       "  736,\n",
       "  87434,\n",
       "  1855,\n",
       "  235269,\n",
       "  578,\n",
       "  665,\n",
       "  13807,\n",
       "  692,\n",
       "  685,\n",
       "  476,\n",
       "  4034,\n",
       "  235265,\n",
       "  62122,\n",
       "  235269,\n",
       "  692,\n",
       "  7554,\n",
       "  476,\n",
       "  7214,\n",
       "  635,\n",
       "  576,\n",
       "  2611,\n",
       "  160483,\n",
       "  774,\n",
       "  476,\n",
       "  16990,\n",
       "  31373,\n",
       "  235269,\n",
       "  47538,\n",
       "  573,\n",
       "  8566,\n",
       "  576,\n",
       "  476,\n",
       "  5915,\n",
       "  15704,\n",
       "  26711,\n",
       "  115351,\n",
       "  731,\n",
       "  49859,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  5688,\n",
       "  573,\n",
       "  15704,\n",
       "  102079,\n",
       "  235269,\n",
       "  11637,\n",
       "  671,\n",
       "  6312,\n",
       "  611,\n",
       "  573,\n",
       "  39221,\n",
       "  604,\n",
       "  1089,\n",
       "  5736,\n",
       "  28730,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  1877,\n",
       "  692,\n",
       "  102079,\n",
       "  5688,\n",
       "  573,\n",
       "  15704,\n",
       "  235269,\n",
       "  692,\n",
       "  7554,\n",
       "  573,\n",
       "  204310,\n",
       "  576,\n",
       "  7907,\n",
       "  16990,\n",
       "  235269,\n",
       "  24830,\n",
       "  573,\n",
       "  8566,\n",
       "  576,\n",
       "  2775,\n",
       "  120076,\n",
       "  575,\n",
       "  573,\n",
       "  1362,\n",
       "  31453,\n",
       "  235265,\n",
       "  4560,\n",
       "  235269,\n",
       "  861,\n",
       "  6137,\n",
       "  9168,\n",
       "  14779,\n",
       "  611,\n",
       "  573,\n",
       "  15704,\n",
       "  685,\n",
       "  692,\n",
       "  6378,\n",
       "  921,\n",
       "  577,\n",
       "  2174,\n",
       "  665,\n",
       "  235265,\n",
       "  33090,\n",
       "  235269,\n",
       "  692,\n",
       "  1717,\n",
       "  476,\n",
       "  81095,\n",
       "  576,\n",
       "  21915,\n",
       "  98465,\n",
       "  578,\n",
       "  476,\n",
       "  2301,\n",
       "  49276,\n",
       "  10751,\n",
       "  476,\n",
       "  38267,\n",
       "  576,\n",
       "  166962,\n",
       "  4394,\n",
       "  22799,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  1987,\n",
       "  573,\n",
       "  21915,\n",
       "  98465,\n",
       "  578,\n",
       "  4394,\n",
       "  22799,\n",
       "  235269,\n",
       "  1300,\n",
       "  9483,\n",
       "  1174,\n",
       "  54829,\n",
       "  575,\n",
       "  970,\n",
       "  3386,\n",
       "  235265,\n",
       "  5040,\n",
       "  235269,\n",
       "  590,\n",
       "  11958,\n",
       "  577,\n",
       "  1987,\n",
       "  476,\n",
       "  4318,\n",
       "  577,\n",
       "  2066,\n",
       "  578,\n",
       "  62487,\n",
       "  970,\n",
       "  7268,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  48000,\n",
       "  476,\n",
       "  10750,\n",
       "  7039,\n",
       "  71557,\n",
       "  573,\n",
       "  90588,\n",
       "  39221,\n",
       "  576,\n",
       "  573,\n",
       "  115885,\n",
       "  9309,\n",
       "  235269,\n",
       "  692,\n",
       "  5045,\n",
       "  1706,\n",
       "  577,\n",
       "  2066,\n",
       "  235265,\n",
       "  1877,\n",
       "  692,\n",
       "  58980,\n",
       "  774,\n",
       "  573,\n",
       "  21915,\n",
       "  98465,\n",
       "  235269,\n",
       "  692,\n",
       "  2375,\n",
       "  1277,\n",
       "  84067,\n",
       "  1384,\n",
       "  5701,\n",
       "  1900,\n",
       "  16659,\n",
       "  1593,\n",
       "  861,\n",
       "  40832,\n",
       "  235269,\n",
       "  82714,\n",
       "  7831,\n",
       "  861,\n",
       "  66547,\n",
       "  235265,\n",
       "  714,\n",
       "  12446,\n",
       "  9788,\n",
       "  576,\n",
       "  4460,\n",
       "  3658,\n",
       "  476,\n",
       "  68489,\n",
       "  51711,\n",
       "  685,\n",
       "  692,\n",
       "  1987,\n",
       "  476,\n",
       "  4318,\n",
       "  577,\n",
       "  27939,\n",
       "  607,\n",
       "  774,\n",
       "  861,\n",
       "  10734,\n",
       "  6319,\n",
       "  2166,\n",
       "  235265,\n",
       "  1877,\n",
       "  692,\n",
       "  12523,\n",
       "  577,\n",
       "  5374,\n",
       "  861,\n",
       "  18599,\n",
       "  235269,\n",
       "  692,\n",
       "  30993,\n",
       "  1163,\n",
       "  696,\n",
       "  573,\n",
       "  43314,\n",
       "  235269,\n",
       "  948,\n",
       "  12353,\n",
       "  16990,\n",
       "  235269,\n",
       "  11041,\n",
       "  1163,\n",
       "  692,\n",
       "  2484,\n",
       "  28786,\n",
       "  235265,\n",
       "  1165,\n",
       "  4930,\n",
       "  674,\n",
       "  692,\n",
       "  791,\n",
       "  1942,\n",
       "  476,\n",
       "  1382,\n",
       "  51305,\n",
       "  575,\n",
       "  736,\n",
       "  87434,\n",
       "  27988,\n",
       "  235265,\n",
       "  3279,\n",
       "  37585,\n",
       "  17564,\n",
       "  235269,\n",
       "  692,\n",
       "  9407,\n",
       "  577,\n",
       "  861,\n",
       "  5368,\n",
       "  235269,\n",
       "  5628,\n",
       "  577,\n",
       "  3142,\n",
       "  9550,\n",
       "  12382,\n",
       "  5674,\n",
       "  9107,\n",
       "  575,\n",
       "  573,\n",
       "  115885,\n",
       "  9309,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  3279,\n",
       "  861,\n",
       "  206608,\n",
       "  31565,\n",
       "  235269,\n",
       "  573,\n",
       "  43314,\n",
       "  235269,\n",
       "  692,\n",
       "  3379,\n",
       "  86304,\n",
       "  1593,\n",
       "  573,\n",
       "  115885,\n",
       "  9309,\n",
       "  235265,\n",
       "  714,\n",
       "  8195,\n",
       "  3307,\n",
       "  577,\n",
       "  59039,\n",
       "  30328,\n",
       "  685,\n",
       "  692,\n",
       "  10734,\n",
       "  22583,\n",
       "  1280,\n",
       "  573,\n",
       "  3760,\n",
       "  576,\n",
       "  573,\n",
       "  87434,\n",
       "  24350,\n",
       "  235265,\n",
       "  29598,\n",
       "  235269,\n",
       "  692,\n",
       "  2063,\n",
       "  3054,\n",
       "  476,\n",
       "  13004,\n",
       "  3703,\n",
       "  146838,\n",
       "  731,\n",
       "  121048,\n",
       "  12387,\n",
       "  8195,\n",
       "  235269,\n",
       "  1024,\n",
       "  17170,\n",
       "  148655,\n",
       "  577,\n",
       "  1736,\n",
       "  476,\n",
       "  4158,\n",
       "  2951,\n",
       "  1677,\n",
       "  36146,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  5374,\n",
       "  3731,\n",
       "  573,\n",
       "  3703,\n",
       "  235269,\n",
       "  23213,\n",
       "  14838,\n",
       "  604,\n",
       "  1089,\n",
       "  11704,\n",
       "  576,\n",
       "  8416,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  1877,\n",
       "  692,\n",
       "  31201,\n",
       "  573,\n",
       "  48806,\n",
       "  3703,\n",
       "  235269,\n",
       "  692,\n",
       "  3365,\n",
       "  157423,\n",
       "  576,\n",
       "  100522,\n",
       "  28223,\n",
       "  64256,\n",
       "  574,\n",
       "  1865,\n",
       "  573,\n",
       "  8195,\n",
       "  235269,\n",
       "  1024,\n",
       "  7539,\n",
       "  115351,\n",
       "  731,\n",
       "  573,\n",
       "  41471,\n",
       "  33812,\n",
       "  235265,\n",
       "  714,\n",
       "  2681,\n",
       "  603,\n",
       "  8028,\n",
       "  675,\n",
       "  573,\n",
       "  41216,\n",
       "  576,\n",
       "  48834,\n",
       "  578,\n",
       "  195417,\n",
       "  235269,\n",
       "  10480,\n",
       "  577,\n",
       "  573,\n",
       "  78659,\n",
       "  73508,\n",
       "  576,\n",
       "  573,\n",
       "  9309,\n",
       "  235265,\n",
       "  62122,\n",
       "  235269,\n",
       "  692,\n",
       "  124386,\n",
       "  3054,\n",
       "  476,\n",
       "  24651,\n",
       "  12559,\n",
       "  235516,\n",
       "  231530,\n",
       "  235250,\n",
       "  2301,\n",
       "  37003,\n",
       "  90266,\n",
       "  675,\n",
       "  23148,\n",
       "  9148,\n",
       "  578,\n",
       "  56189,\n",
       "  33393,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  102079,\n",
       "  5688,\n",
       "  573,\n",
       "  37003,\n",
       "  235269,\n",
       "  106328,\n",
       "  731,\n",
       "  1277,\n",
       "  1156,\n",
       "  192464,\n",
       "  9281,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  1877,\n",
       "  692,\n",
       "  4065,\n",
       "  1280,\n",
       "  573,\n",
       "  37003,\n",
       "  235269,\n",
       "  692,\n",
       "  2375,\n",
       "  476,\n",
       "  5229,\n",
       "  576,\n",
       "  145748,\n",
       "  9903,\n",
       "  1163,\n",
       "  692,\n",
       "  235269,\n",
       "  685,\n",
       "  1013,\n",
       "  573,\n",
       "  1508,\n",
       "  2681,\n",
       "  603,\n",
       "  188458,\n",
       "  675,\n",
       "  11908,\n",
       "  235265,\n",
       "  714,\n",
       "  9148,\n",
       "  46507,\n",
       "  26725,\n",
       "  575,\n",
       "  573,\n",
       "  40567,\n",
       "  235269,\n",
       "  1024,\n",
       "  42536,\n",
       "  56396,\n",
       "  31864,\n",
       "  476,\n",
       "  63602,\n",
       "  143755,\n",
       "  576,\n",
       "  9276,\n",
       "  4492,\n",
       "  573,\n",
       "  9309,\n",
       "  6784,\n",
       "  235265,\n",
       "  878,\n",
       "  573,\n",
       "  5086,\n",
       "  576,\n",
       "  573,\n",
       "  37003,\n",
       "  235269,\n",
       "  692,\n",
       "  7039,\n",
       "  476,\n",
       "  166962,\n",
       "  8719,\n",
       "  576,\n",
       "  2003,\n",
       "  235269,\n",
       "  1277,\n",
       "  4955,\n",
       "  1026,\n",
       "  61307,\n",
       "  675,\n",
       "  184467,\n",
       "  2611,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  102079,\n",
       "  5688,\n",
       "  573,\n",
       "  8719,\n",
       "  578,\n",
       "  25936,\n",
       "  1280,\n",
       "  1277,\n",
       "  39034,\n",
       "  235269,\n",
       "  17978,\n",
       "  1105,\n",
       "  1277,\n",
       "  87434,\n",
       "  6945,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  1877,\n",
       "  692,\n",
       "  19163,\n",
       "  13951,\n",
       "  577,\n",
       "  573,\n",
       "  8719,\n",
       "  235269,\n",
       "  692,\n",
       "  3365,\n",
       "  12559,\n",
       "  576,\n",
       "  861,\n",
       "  22915,\n",
       "  44203,\n",
       "  1355,\n",
       "  696,\n",
       "  692,\n",
       "  235269,\n",
       "  901,\n",
       "  1104,\n",
       "  603,\n",
       "  2775,\n",
       "  2167,\n",
       "  1105,\n",
       "  665,\n",
       "  235516,\n",
       "  231530,\n",
       "  235250,\n",
       "  33464,\n",
       "  29436,\n",
       "  576,\n",
       "  11908,\n",
       "  87737,\n",
       "  861,\n",
       "  2416,\n",
       "  235269,\n",
       "  685,\n",
       "  1013,\n",
       "  573,\n",
       "  9309,\n",
       "  5344,\n",
       "  603,\n",
       "  23913,\n",
       "  575,\n",
       "  861,\n",
       "  4628,\n",
       "  235265,\n",
       "  62122,\n",
       "  235269,\n",
       "  692,\n",
       "  7554,\n",
       "  8069,\n",
       "  21776,\n",
       "  573,\n",
       "  4955,\n",
       "  576,\n",
       "  573,\n",
       "  2003,\n",
       "  235269,\n",
       "  476,\n",
       "  132889,\n",
       "  574,\n",
       "  7479,\n",
       "  64256,\n",
       "  574,\n",
       "  577,\n",
       "  578,\n",
       "  9750,\n",
       "  675,\n",
       "  70318,\n",
       "  90140,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  6378,\n",
       "  921,\n",
       "  577,\n",
       "  6437,\n",
       "  573,\n",
       "  2003,\n",
       "  235269,\n",
       "  17978,\n",
       "  1105,\n",
       "  573,\n",
       "  27988,\n",
       "  2819,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  154506,\n",
       "  8254,\n",
       "  1877,\n",
       "  861,\n",
       "  18976,\n",
       "  16141,\n",
       "  2691,\n",
       "  573,\n",
       "  4955,\n",
       "  576,\n",
       "  573,\n",
       "  2003,\n",
       "  235269,\n",
       "  573,\n",
       "  132889,\n",
       "  574,\n",
       "  7479,\n",
       "  73643,\n",
       "  774,\n",
       "  21776,\n",
       "  573,\n",
       "  4955,\n",
       "  235516,\n",
       "  231530,\n",
       "  235250,\n",
       "  74832,\n",
       "  2003,\n",
       "  48477,\n",
       "  675,\n",
       "  166962,\n",
       "  18751,\n",
       "  578,\n",
       "  85534,\n",
       "  4628,\n",
       "  235265,\n",
       "  1165,\n",
       "  48660,\n",
       "  833,\n",
       "  39466,\n",
       "  578,\n",
       "  107625,\n",
       "  2449,\n",
       "  692,\n",
       "  575,\n",
       "  476,\n",
       "  74832,\n",
       "  11877,\n",
       "  235269,\n",
       "  10317,\n",
       "  30999,\n",
       "  576,\n",
       "  56189,\n",
       "  93625,\n",
       "  575,\n",
       "  1277,\n",
       "  18757,\n",
       "  235265,\n",
       "  109,\n",
       "  235309,\n",
       "  7324,\n",
       "  8254,\n",
       "  590,\n",
       "  11491,\n",
       "  696,\n",
       "  573,\n",
       "  2003,\n",
       "  48477,\n",
       "  578,\n",
       "  3255,\n",
       "  665,\n",
       "  476,\n",
       "  9376,\n",
       "  32338,\n",
       "  235269,\n",
       "  18360,\n",
       "  ...]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_dataset['text'][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed32c6aa8e994fcc9493ac42b414aa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adito\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:555: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to unscale FP16 gradients.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adito\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:331\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 331\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32mc:\\Users\\adito\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adito\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2003\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1998\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m   1999\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[0;32m   2000\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[0;32m   2001\u001b[0m     )\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2003\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2005\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2009\u001b[0m     is_accelerate_available()\n\u001b[0;32m   2010\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2011\u001b[0m ):\n\u001b[0;32m   2012\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[1;32mc:\\Users\\adito\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:2101\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[1;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   2098\u001b[0m     \u001b[38;5;66;03m# `accelerator.backward(loss)` is doing that automatically. Therefore, its implementation is not needed\u001b[39;00m\n\u001b[0;32m   2099\u001b[0m     \u001b[38;5;66;03m# We cannot return the gradient norm because DeepSpeed does it.\u001b[39;00m\n\u001b[0;32m   2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[1;32mc:\\Users\\adito\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:2064\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m   2062\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m xm\u001b[38;5;241m.\u001b[39m_fetch_gradients(opt)\n\u001b[0;32m   2063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(gradients, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_processes)\n\u001b[1;32m-> 2064\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adito\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:336\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    333\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    334\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 336\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[1;32mc:\\Users\\adito\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:258\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[1;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_fp16) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to unscale FP16 gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[1;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
