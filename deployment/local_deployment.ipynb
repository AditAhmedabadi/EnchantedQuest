{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f72f92c11f4a87aa4bf71c5b0ae11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the environment file\n",
    "load_dotenv(\"token.env\")\n",
    "\n",
    "# Retrieve the API token\n",
    "api_token = os.getenv(\"api_token\")\n",
    "\n",
    "login(token = api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'do_sample' : True,\n",
    "    'temperature': 0,\n",
    "    'max_length' : 1000\n",
    "}\n",
    "\n",
    "pipeline_kwargs = {\n",
    "    'max_new_tokens' : 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47444b4c8379470fa5cda09d1b4e8b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'google/gemma-7b-it'\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type = 'nf4',\n",
    "#     bnb_4bit_compute_dtype = torch.float16\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16 , trust_remote_code = True, device_map = 'cuda')\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
    "hf = HuggingFacePipeline(pipeline=pipe, model_kwargs = model_kwargs, pipeline_kwargs = pipeline_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is this'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('What is this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is electroencephalography?\n",
      "\n",
      "Answer: Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables = ['question'])\n",
    "\n",
    "llm_chain = LLMChain(prompt = prompt, llm = hf, verbose = True)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "answer = llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a friendly chatbot assistant that responds conversationally to users' questions.\n",
    "Keep the answers short, unless specifically asked by the user to elaborate on something.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    result = llm_chain(question)\n",
    "    print(result['question'])\n",
    "    print(\"\")\n",
    "    print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question('Describe yourself')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "               model=model, \n",
    "               tokenizer=tokenizer, \n",
    "               max_new_tokens=512,\n",
    "               do_sample=True,\n",
    "               temperature=1,\n",
    "               )\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "query = \"Who is Shah Rukh Khan?\"\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "query = \"describe youself\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### System:\n",
    "You are an AI assistant that follows instruction extremely well. \n",
    "Help as much as you can. Please be truthful and give direct answers\n",
    "\n",
    "### User:\n",
    "{query}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "response = hf.predict(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a friendly chatbot assistant that responds conversationally to users' questions.\n",
    "Keep the answers short, unless specifically asked by the user to elaborate on something.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescribe Famous Landmarks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gemma/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gemma/lib/python3.10/site-packages/transformers/generation/utils.py:1376\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1374\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1375\u001b[0m )\n\u001b[0;32m-> 1376\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39moutput_attentions\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "tokeni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    result = llm_chain.invoke(question)\n",
    "    print(result['question'])\n",
    "    print(\"\")\n",
    "    print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TimerError(Exception):\n",
    "    ''' A custom excpetion used to report errors in use of Timer Class'''\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        if self._start_time is not None:\n",
    "            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self._start_time is None:\n",
    "            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        print(f'Elapsed Time: {elapsed_time:0.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe some famous landmarks in London\n",
      "\n",
      " Describe landmarks\n",
      "Elapsed Time: 1.6967 seconds\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    ask_question(\"Describe some famous landmarks in London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe some famous anime characters\n",
      "****************************************************************************************************\n",
      " Describe some famous anime characters\n",
      "Elapsed Time: 0.4790 seconds\n"
     ]
    }
   ],
   "source": [
    "ask_question('Describe some famous anime characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
