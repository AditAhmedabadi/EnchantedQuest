{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adit/anaconda3/envs/gemma/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/adit/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from apply_format import template_from_dir\n",
    "from datasets import Dataset\n",
    "# Load the environment file\n",
    "load_dotenv(\"token.env\")\n",
    "\n",
    "# Retrieve the API token\n",
    "api_token = os.getenv(\"api_token\")\n",
    "\n",
    "login(api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f'Number of GPUs available: {num_gpus}')\n",
    "\n",
    "    # Print the name of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "else:\n",
    "    print('No GPUs available, running on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.56s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config, token = api_token, trust_remote_code = True)\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = api_token, trust_remote_code = True)\n",
    "tokenizer.padding_side = 'right'\n",
    "# tokenizer.pad_token = eos\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_data = template_from_dir('data')\n",
    "len(convo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 113/113 [00:00<00:00, 1129.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Play a game with me where i am in an enchanted forest full of beasts and loots. You be the narrator and i will be the player, play a dialog game with me.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "[Narrator]: In the heart of the enchanted forest, a fierce band of goblins emerges from the shadows, their eyes filled with malice as they brandish their weapons.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "[Player]: Fight the goblins.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "[Narrator]: With determination, you charge into battle, your weapon at the ready. As the goblins close in, you strike with swift and precise blows, fending off their attacks with skill and agility.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "[Player]: Dodge the goblins' strikes.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "[Narrator]: With nimble footwork, you evade the goblins' swinging weapons, dodging their strikes and keeping yourself out of harm's way. With each dodge, you create openings for counterattacks.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "[Player]: Attack the goblins' weak points.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "[Narrator]: Spotting vulnerabilities in the goblins' defenses, you strike with precision, targeting their weak spots and swiftly dispatching them one by one. As the last goblin falls, you emerge victorious in the heart of the enchanted forest.[Game Over]: With the goblins defeated, you take a moment to catch your breath, knowing that your bravery has saved the forest from their menace. Your victory will be remembered for generations to come.<end_of_turn>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_dict({\"chat\": convo_data})\n",
    "dataset = dataset.map(lambda x: {\"messages\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n",
    "dataset = dataset.remove_columns('chat')\n",
    "print(dataset['messages'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 113\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 101\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 12\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model,TaskType\n",
    "\n",
    "lora_alpha = 100\n",
    "lora_dropout = 0.01\n",
    "lora_r = 100\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = lora_alpha,\n",
    "    lora_dropout = lora_dropout,\n",
    "    r = lora_r,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results-alt\"\n",
    "gradient_accumulation_steps = 5\n",
    "save_steps = 10\n",
    "logging_steps = 5\n",
    "gradient_checkpointing = True\n",
    "optim = \"paged_adamw_32bit\"\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "weight_decay = 0.01\n",
    "max_steps = 100\n",
    "# num_train_epochs = 20\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    # load_best_model_at_end=True,\n",
    "    optim = optim,\n",
    "    # num_train_epochs = num_train_epochs,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    save_steps = save_steps,\n",
    "    logging_steps = logging_steps,\n",
    "    auto_find_batch_size=True,\n",
    "    # per_device_train_batch_size=8,\n",
    "    learning_rate = learning_rate,\n",
    "    max_grad_norm = max_grad_norm,\n",
    "    fp16 = True,\n",
    "    max_steps = max_steps,\n",
    "    group_by_length = True,\n",
    "    gradient_checkpointing = True,\n",
    "    weight_decay = weight_decay,\n",
    "    lr_scheduler_type = lr_scheduler_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 524363776\n",
      "all model parameters: 1515268096\n",
      "percentage of trainable model parameters: 34.61%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 15 examples [00:00, 233.25 examples/s]\n",
      "Generating train split: 1 examples [00:00, 229.89 examples/s]\n",
      "/home/adit/anaconda3/envs/gemma/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/adit/anaconda3/envs/gemma/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2874\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field=\"messages\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    "    packing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maditobito\u001b[0m (\u001b[33madit_ahmedabadi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/projects/game/wandb/run-20240309_121517-54iphzu8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adit_ahmedabadi/huggingface/runs/54iphzu8' target=\"_blank\">sleek-dew-41</a></strong> to <a href='https://wandb.ai/adit_ahmedabadi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adit_ahmedabadi/huggingface' target=\"_blank\">https://wandb.ai/adit_ahmedabadi/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adit_ahmedabadi/huggingface/runs/54iphzu8' target=\"_blank\">https://wandb.ai/adit_ahmedabadi/huggingface/runs/54iphzu8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adit/anaconda3/envs/gemma/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/100 : < :, Epoch 0.62/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed. \n",
      "Saving the model.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adit/anaconda3/envs/gemma/lib/python3.10/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 02be03d3-3bab-4d47-93f9-c91703d45b41)') - silently ignoring the lookup for the file config.json in google/gemma-2b-it.\n",
      "  warnings.warn(\n",
      "/home/adit/anaconda3/envs/gemma/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n",
      "Model and trainer deleted from memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model loaded successfully\n",
      "Model merged successfully\n",
      "Saving the merged model\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Completed. \\nSaving the model.....\")\n",
    "\n",
    "trainer.save_model(training_args.output_dir)\n",
    "\n",
    "print(\"Model saved successfully\")\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Model and trainer deleted from memory\")\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(training_args.output_dir, token=api_token, trust_remote_code=True)\n",
    "\n",
    "print(\"PEFT Model loaded successfully\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "print(\"Model merged successfully\")\n",
    "\n",
    "print(\"Saving the merged model\")\n",
    "merge_output_dir = './2b-it-peft-alt'\n",
    "merged_model.save_pretrained(merge_output_dir, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
